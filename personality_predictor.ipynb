{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "509ffd9e-64be-423a-92c4-1cab9bc8949e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /Users/nageshs/miniconda3/lib/python3.12/site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /Users/nageshs/miniconda3/lib/python3.12/site-packages (from pandas) (2.2.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/nageshs/miniconda3/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/nageshs/miniconda3/lib/python3.12/site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/nageshs/miniconda3/lib/python3.12/site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/nageshs/miniconda3/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0e959b80-8f85-4f1e-a266-3e3f3cd604ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 rows of the dataset:\n",
      "   EXT1  EXT2  EXT3  EXT4  EXT5  EXT6  EXT7  EXT8  EXT9  EXT10  ...  \\\n",
      "0   4.0   1.0   5.0   2.0   5.0   1.0   5.0   2.0   4.0    1.0  ...   \n",
      "1   3.0   5.0   3.0   4.0   3.0   3.0   2.0   5.0   1.0    5.0  ...   \n",
      "2   2.0   3.0   4.0   4.0   3.0   2.0   1.0   3.0   2.0    5.0  ...   \n",
      "3   2.0   2.0   2.0   3.0   4.0   2.0   2.0   4.0   1.0    4.0  ...   \n",
      "4   3.0   3.0   3.0   3.0   5.0   3.0   3.0   5.0   3.0    4.0  ...   \n",
      "\n",
      "              dateload  screenw  screenh  introelapse  testelapse  endelapse  \\\n",
      "0  2016-03-03 02:01:01    768.0   1024.0          9.0       234.0          6   \n",
      "1  2016-03-03 02:01:20   1360.0    768.0         12.0       179.0         11   \n",
      "2  2016-03-03 02:01:56   1366.0    768.0          3.0       186.0          7   \n",
      "3  2016-03-03 02:02:02   1920.0   1200.0        186.0       219.0          7   \n",
      "4  2016-03-03 02:02:57   1366.0    768.0          8.0       315.0         17   \n",
      "\n",
      "   IPC  country  lat_appx_lots_of_err  long_appx_lots_of_err  \n",
      "0    1       GB               51.5448                 0.1991  \n",
      "1    1       MY                3.1698                101.706  \n",
      "2    1       GB               54.9119                -1.3833  \n",
      "3    1       GB                 51.75                  -1.25  \n",
      "4    2       KE                   1.0                   38.0  \n",
      "\n",
      "[5 rows x 110 columns]\n",
      "\n",
      "Dataset Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1015341 entries, 0 to 1015340\n",
      "Columns: 110 entries, EXT1 to long_appx_lots_of_err\n",
      "dtypes: float64(104), int64(2), object(4)\n",
      "memory usage: 852.1+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Correctly parse the tab-separated file\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Update the file path if necessary\n",
    "file_path = \"data-final.csv\"\n",
    "\n",
    "# Load the dataset with the correct delimiter\n",
    "df = pd.read_csv(file_path, delimiter='\\t')\n",
    "\n",
    "# Display the first few rows to check the separation\n",
    "print(\"First 5 rows of the dataset:\")\n",
    "print(df.head())\n",
    "\n",
    "# Show the dataset info to verify that columns are correctly separated\n",
    "print(\"\\nDataset Info:\")\n",
    "print(df.info())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc40a8ee-8cf0-4807-8959-b318a8cfd2a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 rows of the dataset:\n",
      "   EXT1  EXT2  EXT3  EXT4  EXT5  EXT6  EXT7  EXT8  EXT9  EXT10  ...  \\\n",
      "0   4.0   1.0   5.0   2.0   5.0   1.0   5.0   2.0   4.0    1.0  ...   \n",
      "1   3.0   5.0   3.0   4.0   3.0   3.0   2.0   5.0   1.0    5.0  ...   \n",
      "2   2.0   3.0   4.0   4.0   3.0   2.0   1.0   3.0   2.0    5.0  ...   \n",
      "3   2.0   2.0   2.0   3.0   4.0   2.0   2.0   4.0   1.0    4.0  ...   \n",
      "4   3.0   3.0   3.0   3.0   5.0   3.0   3.0   5.0   3.0    4.0  ...   \n",
      "\n",
      "              dateload  screenw  screenh  introelapse  testelapse  endelapse  \\\n",
      "0  2016-03-03 02:01:01    768.0   1024.0          9.0       234.0          6   \n",
      "1  2016-03-03 02:01:20   1360.0    768.0         12.0       179.0         11   \n",
      "2  2016-03-03 02:01:56   1366.0    768.0          3.0       186.0          7   \n",
      "3  2016-03-03 02:02:02   1920.0   1200.0        186.0       219.0          7   \n",
      "4  2016-03-03 02:02:57   1366.0    768.0          8.0       315.0         17   \n",
      "\n",
      "   IPC  country  lat_appx_lots_of_err  long_appx_lots_of_err  \n",
      "0    1       GB               51.5448                 0.1991  \n",
      "1    1       MY                3.1698                101.706  \n",
      "2    1       GB               54.9119                -1.3833  \n",
      "3    1       GB                 51.75                  -1.25  \n",
      "4    2       KE                   1.0                   38.0  \n",
      "\n",
      "[5 rows x 110 columns]\n",
      "\n",
      "Dataset Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1015341 entries, 0 to 1015340\n",
      "Columns: 110 entries, EXT1 to long_appx_lots_of_err\n",
      "dtypes: float64(104), int64(2), object(4)\n",
      "memory usage: 852.1+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Correctly parse the tab-separated file\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Update the file path if necessary\n",
    "file_path = \"data-final.csv\"\n",
    "\n",
    "# Load the dataset with the correct delimiter\n",
    "df = pd.read_csv(file_path, delimiter='\\t')\n",
    "\n",
    "# Display the first few rows to check the separation\n",
    "print(\"First 5 rows of the dataset:\")\n",
    "print(df.head())\n",
    "\n",
    "# Show the dataset info to verify that columns are correctly separated\n",
    "print(\"\\nDataset Info:\")\n",
    "print(df.info())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "53b51cd8-4b8b-487b-9111-c1461bcbd481",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 rows of computed personality scores:\n",
      "   Extraversion  Emotional_Stability  Agreeableness  Conscientiousness  \\\n",
      "0           3.0                  2.4            3.1                3.2   \n",
      "1           3.4                  2.1            3.2                3.1   \n",
      "2           2.9                  2.6            2.8                2.8   \n",
      "3           2.6                  2.7            3.2                2.7   \n",
      "4           3.5                  2.3            3.0                3.2   \n",
      "\n",
      "   Openness  \n",
      "0       3.3  \n",
      "1       2.7  \n",
      "2       3.1  \n",
      "3       3.1  \n",
      "4       3.6  \n"
     ]
    }
   ],
   "source": [
    "# Step 3: Compute Aggregate Personality Scores\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming df is already loaded with the correct delimiter as in Step 2.\n",
    "\n",
    "# Define the columns for each personality dimension\n",
    "ext_cols = [f\"EXT{i}\" for i in range(1, 11)]\n",
    "est_cols = [f\"EST{i}\" for i in range(1, 11)]\n",
    "agr_cols = [f\"AGR{i}\" for i in range(1, 11)]\n",
    "csn_cols = [f\"CSN{i}\" for i in range(1, 11)]\n",
    "opn_cols = [f\"OPN{i}\" for i in range(1, 11)]\n",
    "\n",
    "# Calculate the average score for each personality dimension\n",
    "df['Extraversion'] = df[ext_cols].mean(axis=1)\n",
    "df['Emotional_Stability'] = df[est_cols].mean(axis=1)\n",
    "df['Agreeableness'] = df[agr_cols].mean(axis=1)\n",
    "df['Conscientiousness'] = df[csn_cols].mean(axis=1)\n",
    "df['Openness'] = df[opn_cols].mean(axis=1)\n",
    "\n",
    "# Display the first few rows of the new personality scores\n",
    "print(\"First 5 rows of computed personality scores:\")\n",
    "print(df[['Extraversion', 'Emotional_Stability', 'Agreeableness', 'Conscientiousness', 'Openness']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c6e360a9-e196-4f53-91ae-a13ca4be5bbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Personality report for the first entry:\n",
      "You have a balanced level of extraversion. You may experience emotional ups and downs. You are highly agreeable and cooperative. You are very conscientious, organized, and reliable. You are open-minded and curious about new experiences.\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Generate a Personality Report Text from Aggregated Scores\n",
    "\n",
    "def personality_report(row):\n",
    "    report = \"\"\n",
    "    \n",
    "    # Extraversion\n",
    "    if row['Extraversion'] < 3:\n",
    "        report += \"You tend to be more introverted. \"\n",
    "    elif row['Extraversion'] > 3:\n",
    "        report += \"You are quite extraverted. \"\n",
    "    else:\n",
    "        report += \"You have a balanced level of extraversion. \"\n",
    "    \n",
    "    # Emotional Stability\n",
    "    if row['Emotional_Stability'] < 3:\n",
    "        report += \"You may experience emotional ups and downs. \"\n",
    "    elif row['Emotional_Stability'] > 3:\n",
    "        report += \"You are emotionally stable and resilient. \"\n",
    "    else:\n",
    "        report += \"Your emotional stability is average. \"\n",
    "    \n",
    "    # Agreeableness\n",
    "    if row['Agreeableness'] < 3:\n",
    "        report += \"You tend to be less agreeable, perhaps more competitive. \"\n",
    "    elif row['Agreeableness'] > 3:\n",
    "        report += \"You are highly agreeable and cooperative. \"\n",
    "    else:\n",
    "        report += \"You have a balanced level of agreeableness. \"\n",
    "    \n",
    "    # Conscientiousness\n",
    "    if row['Conscientiousness'] < 3:\n",
    "        report += \"You might be more spontaneous and less structured. \"\n",
    "    elif row['Conscientiousness'] > 3:\n",
    "        report += \"You are very conscientious, organized, and reliable. \"\n",
    "    else:\n",
    "        report += \"Your conscientiousness is moderate. \"\n",
    "    \n",
    "    # Openness\n",
    "    if row['Openness'] < 3:\n",
    "        report += \"You prefer familiar experiences over new adventures.\"\n",
    "    elif row['Openness'] > 3:\n",
    "        report += \"You are open-minded and curious about new experiences.\"\n",
    "    else:\n",
    "        report += \"You have a balanced level of openness.\"\n",
    "    \n",
    "    return report\n",
    "\n",
    "# Test the function on the first row of the dataset\n",
    "print(\"Personality report for the first entry:\")\n",
    "print(personality_report(df.iloc[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "699de6d7-c48f-47f1-a517-eb5677f987a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, TrainingArguments, Trainer, DataCollatorForSeq2Seq\n",
    "\n",
    "# ---------------------------\n",
    "# Step 1: Load and Prepare the Data\n",
    "# ---------------------------\n",
    "file_path = \"data-final.csv\"\n",
    "df = pd.read_csv(file_path, delimiter='\\t')\n",
    "\n",
    "# Define the columns for each personality dimension\n",
    "ext_cols = [f\"EXT{i}\" for i in range(1, 11)]\n",
    "est_cols = [f\"EST{i}\" for i in range(1, 11)]\n",
    "agr_cols = [f\"AGR{i}\" for i in range(1, 11)]\n",
    "csn_cols = [f\"CSN{i}\" for i in range(1, 11)]\n",
    "opn_cols = [f\"OPN{i}\" for i in range(1, 11)]\n",
    "\n",
    "# Compute aggregate personality scores (as before)\n",
    "df['Extraversion'] = df[ext_cols].mean(axis=1)\n",
    "df['Emotional_Stability'] = df[est_cols].mean(axis=1)\n",
    "df['Agreeableness'] = df[agr_cols].mean(axis=1)\n",
    "df['Conscientiousness'] = df[csn_cols].mean(axis=1)\n",
    "df['Openness'] = df[opn_cols].mean(axis=1)\n",
    "\n",
    "# ---------------------------\n",
    "# Step 2: Create Training Examples\n",
    "# ---------------------------\n",
    "def create_example(row):\n",
    "    # Create an input prompt from the raw responses.\n",
    "    # Here we simply list all the responses from the relevant columns.\n",
    "    responses = []\n",
    "    for col in ext_cols + est_cols + agr_cols + csn_cols + opn_cols:\n",
    "        responses.append(f\"{col}: {row[col]}\")\n",
    "    input_text = \"Responses: \" + \", \".join(responses)\n",
    "    \n",
    "    # Create the target text from the aggregated personality scores.\n",
    "    target_text = (\n",
    "        f\"Extraversion: {row['Extraversion']:.1f}, \"\n",
    "        f\"Emotional Stability: {row['Emotional_Stability']:.1f}, \"\n",
    "        f\"Agreeableness: {row['Agreeableness']:.1f}, \"\n",
    "        f\"Conscientiousness: {row['Conscientiousness']:.1f}, \"\n",
    "        f\"Openness: {row['Openness']:.1f}\"\n",
    "    )\n",
    "    return {\"input_text\": input_text, \"target_text\": target_text}\n",
    "\n",
    "# For demonstration, we use a subset of the data (e.g., first 1000 rows)\n",
    "training_examples = df.head(1000).apply(create_example, axis=1).tolist()\n",
    "\n",
    "# Convert the list of examples into a Hugging Face Dataset\n",
    "train_dataset = Dataset.from_list(training_examples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c4ee2702-e081-46ca-b4ee-b16f8f9f8d67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c92c39929a884a40878464f3a4ae6581",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.32k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cab7b54527574f38a8da8f79165f04f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2bf33085fb44e078da80223a535a616",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6489d38fb9c94d01aef942bd629e0c0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3971152ca60345eb82d8cae59349efd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/242M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53164b45f8c041e187d187267f8a90c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08543be9e2024a0d8797c4a3045601e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nageshs/miniconda3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:3970: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='750' max='750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [750/750 03:42, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.387500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.315900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.569200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.119100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.808200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.640000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.564000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.493200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.470400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.458400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.448400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.406300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.455900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.414700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.456700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.414400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.404800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.432400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.406100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.386000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.408800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.406100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>0.403200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.387900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.396500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.377800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>0.417200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.369400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>0.392100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.398300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>0.379200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.364200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>0.409800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>0.383700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.380200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.363800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>0.393300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>0.369700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>0.374200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.385300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>410</td>\n",
       "      <td>0.409700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>0.378500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>430</td>\n",
       "      <td>0.362500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>0.360800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.367300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>0.365400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>470</td>\n",
       "      <td>0.366700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.358900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>490</td>\n",
       "      <td>0.369500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.359300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>510</td>\n",
       "      <td>0.356200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>0.357800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>530</td>\n",
       "      <td>0.360900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>0.338500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.374500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>0.362400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>570</td>\n",
       "      <td>0.398800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>580</td>\n",
       "      <td>0.363600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>590</td>\n",
       "      <td>0.382100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.352000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>610</td>\n",
       "      <td>0.362800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>620</td>\n",
       "      <td>0.370300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>630</td>\n",
       "      <td>0.378100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>0.348000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.360900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>660</td>\n",
       "      <td>0.374200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>670</td>\n",
       "      <td>0.364300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>680</td>\n",
       "      <td>0.350500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>690</td>\n",
       "      <td>0.389900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.356200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>710</td>\n",
       "      <td>0.374400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>0.366500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>730</td>\n",
       "      <td>0.363300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>740</td>\n",
       "      <td>0.364200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.366400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=750, training_loss=0.48863226222991946, metrics={'train_runtime': 224.9626, 'train_samples_per_second': 13.336, 'train_steps_per_second': 3.334, 'total_flos': 283905862533120.0, 'train_loss': 0.48863226222991946, 'epoch': 3.0})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# ---------------------------\n",
    "# Step 3: Fine-Tune a T5 Model for Text-to-Text Regression\n",
    "# ---------------------------\n",
    "model_name = \"t5-small\"  # You can experiment with \"t5-base\" for higher quality.\n",
    "tokenizer_t5 = T5Tokenizer.from_pretrained(model_name)\n",
    "model_t5 = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = examples[\"input_text\"]\n",
    "    targets = examples[\"target_text\"]\n",
    "    model_inputs = tokenizer_t5(inputs, max_length=512, truncation=True)\n",
    "    # Tokenize the targets\n",
    "    with tokenizer_t5.as_target_tokenizer():\n",
    "        labels = tokenizer_t5(targets, max_length=32, truncation=True).input_ids\n",
    "    model_inputs[\"labels\"] = labels\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_dataset = train_dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer_t5, model=model_t5)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./t5-personality\",\n",
    "    num_train_epochs=3,                     # Increase epochs as needed\n",
    "    per_device_train_batch_size=4,          # Adjust based on your hardware\n",
    "    logging_steps=10,\n",
    "    save_steps=50,\n",
    "    save_total_limit=2,\n",
    "    prediction_loss_only=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model_t5,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# Start fine-tuning (this will train the model on your training examples)\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e24660f8-4d37-4b48-adaa-0ad317e3a58c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./t5-personality/tokenizer_config.json',\n",
       " './t5-personality/special_tokens_map.json',\n",
       " './t5-personality/spiece.model',\n",
       " './t5-personality/added_tokens.json')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.save_model(\"./t5-personality\")\n",
    "tokenizer_t5.save_pretrained(\"./t5-personality\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a8b2dbbf-8c73-4379-a039-53aa0554dcb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7868\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7868/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import torch\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 1. Load your fine-tuned T5 model and tokenizer.\n",
    "# (If you haven't saved your fine-tuned model, you can temporarily load a pre-trained model, e.g., \"t5-small\".)\n",
    "model_path = \"./t5-personality\"  # Adjust path if needed.\n",
    "# If the model is not saved, you can use:\n",
    "# model_name = \"t5-small\"\n",
    "# tokenizer_t5 = T5Tokenizer.from_pretrained(model_name)\n",
    "# model_t5 = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "tokenizer_t5 = T5Tokenizer.from_pretrained(model_path)\n",
    "model_t5 = T5ForConditionalGeneration.from_pretrained(model_path)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_t5.to(device)\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 2. Revised personality report function with error handling:\n",
    "def personality_report(scores):\n",
    "    # For each trait, if the score is None, assume a neutral score of 3.\n",
    "    ext = scores.get('Extraversion') if scores.get('Extraversion') is not None else 3\n",
    "    emo = scores.get('Emotional Stability') if scores.get('Emotional Stability') is not None else 3\n",
    "    agr = scores.get('Agreeableness') if scores.get('Agreeableness') is not None else 3\n",
    "    csn = scores.get('Conscientiousness') if scores.get('Conscientiousness') is not None else 3\n",
    "    opn = scores.get('Openness') if scores.get('Openness') is not None else 3\n",
    "\n",
    "    report = \"\"\n",
    "    \n",
    "    # Extraversion\n",
    "    if ext < 3:\n",
    "        report += \"You tend to be more introverted. \"\n",
    "    elif ext > 3:\n",
    "        report += \"You are quite extraverted. \"\n",
    "    else:\n",
    "        report += \"You have a balanced level of extraversion. \"\n",
    "    \n",
    "    # Emotional Stability\n",
    "    if emo < 3:\n",
    "        report += \"You may experience emotional ups and downs. \"\n",
    "    elif emo > 3:\n",
    "        report += \"You are emotionally stable and resilient. \"\n",
    "    else:\n",
    "        report += \"Your emotional stability is average. \"\n",
    "    \n",
    "    # Agreeableness\n",
    "    if agr < 3:\n",
    "        report += \"You tend to be less agreeable, perhaps more competitive. \"\n",
    "    elif agr > 3:\n",
    "        report += \"You are highly agreeable and cooperative. \"\n",
    "    else:\n",
    "        report += \"You have a balanced level of agreeableness. \"\n",
    "    \n",
    "    # Conscientiousness\n",
    "    if csn < 3:\n",
    "        report += \"You might be more spontaneous and less structured. \"\n",
    "    elif csn > 3:\n",
    "        report += \"You are very conscientious, organized, and reliable. \"\n",
    "    else:\n",
    "        report += \"Your conscientiousness is moderate. \"\n",
    "    \n",
    "    # Openness\n",
    "    if opn < 3:\n",
    "        report += \"You prefer familiar experiences over new adventures.\"\n",
    "    elif opn > 3:\n",
    "        report += \"You are open-minded and curious about new experiences.\"\n",
    "    else:\n",
    "        report += \"You have a balanced level of openness.\"\n",
    "    \n",
    "    return report\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 3. Function to predict aggregated personality scores from the questionnaire using the fine-tuned T5 model.\n",
    "def predict_scores_from_questionnaire(q1, q2, q3, q4, q5):\n",
    "    # Construct the prompt using the free-form answers.\n",
    "    input_text = (\n",
    "        f\"Questionnaire responses:\\n\"\n",
    "        f\"Extraversion: {q1}\\n\"\n",
    "        f\"Emotional Stability: {q2}\\n\"\n",
    "        f\"Conscientiousness: {q3}\\n\"\n",
    "        f\"Agreeableness: {q4}\\n\"\n",
    "        f\"Openness: {q5}\\n\\n\"\n",
    "        \"Predict aggregated personality scores in the format: \"\n",
    "        \"Extraversion: X, Emotional Stability: Y, Agreeableness: Z, Conscientiousness: W, Openness: V.\"\n",
    "    )\n",
    "    inputs = tokenizer_t5(input_text, return_tensors=\"pt\", truncation=True, max_length=512).to(device)\n",
    "    outputs = model_t5.generate(**inputs, max_length=50, num_return_sequences=1)\n",
    "    prediction = tokenizer_t5.decode(outputs[0], skip_special_tokens=True)\n",
    "    return prediction\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 4. Function to parse the predicted scores from the model's output.\n",
    "def parse_scores(prediction):\n",
    "    parts = prediction.split(',')\n",
    "    scores = {}\n",
    "    for part in parts:\n",
    "        if ':' in part:\n",
    "            key, val = part.split(':', 1)\n",
    "            try:\n",
    "                scores[key.strip()] = float(val.strip())\n",
    "            except:\n",
    "                scores[key.strip()] = None\n",
    "    return scores\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 5. Main function to generate final personality feedback.\n",
    "def final_personality_feedback(q1, q2, q3, q4, q5):\n",
    "    # Predict aggregated scores using the fine-tuned model.\n",
    "    prediction = predict_scores_from_questionnaire(q1, q2, q3, q4, q5)\n",
    "    scores = parse_scores(prediction)\n",
    "    report = personality_report(scores)\n",
    "    # Format the scores for display.\n",
    "    scores_str = \", \".join([f\"{trait}: {score}\" for trait, score in scores.items()])\n",
    "    final_output = (\n",
    "        f\"Aggregated Scores: {scores_str}\\n\\n\"\n",
    "        f\"Personality Report:\\n{report}\\n\\n\"\n",
    "        f\"(Raw Model Prediction: {prediction})\"\n",
    "    )\n",
    "    return final_output\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 6. Build the Gradio interface with free-form questionnaire inputs.\n",
    "iface = gr.Interface(\n",
    "    fn=final_personality_feedback,\n",
    "    inputs=[\n",
    "        gr.Textbox(lines=3, label=\"Question 1: Describe your social life (Extraversion)\"),\n",
    "        gr.Textbox(lines=3, label=\"Question 2: How do you manage stress and emotions (Emotional Stability)\"),\n",
    "        gr.Textbox(lines=3, label=\"Question 3: Describe your organizational habits (Conscientiousness)\"),\n",
    "        gr.Textbox(lines=3, label=\"Question 4: Tell us about your interactions with others (Agreeableness)\"),\n",
    "        gr.Textbox(lines=3, label=\"Question 5: How open are you to new experiences (Openness)\")\n",
    "    ],\n",
    "    outputs=\"text\",\n",
    "    title=\"Personality Feedback Generator\",\n",
    "    description=(\n",
    "        \"Answer the questions in your own words. Your responses will be processed by a fine-tuned model \"\n",
    "        \"to predict aggregated personality scores, which are then used to generate a detailed personality report.\"\n",
    "    )\n",
    ")\n",
    "\n",
    "iface.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "61c3b337-c6cc-4b4c-bf81-a113ed7d0cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --------------------------------------------------\n",
    "# 3. Function to predict aggregated personality scores using the fine-tuned T5 model.\n",
    "def predict_scores_from_questionnaire(q1, q2, q3, q4, q5):\n",
    "    # Build the prompt using the free-form answers.\n",
    "    # Here we map each question to a trait.\n",
    "    input_text = (\n",
    "        f\"Questionnaire responses:\\n\"\n",
    "        f\"Extraversion: {q1}\\n\"\n",
    "        f\"Emotional Stability: {q2}\\n\"\n",
    "        f\"Conscientiousness: {q3}\\n\"\n",
    "        f\"Agreeableness: {q4}\\n\"\n",
    "        f\"Openness: {q5}\\n\\n\"\n",
    "        \"Predict aggregated personality scores in the format: \"\n",
    "        \"Extraversion: X, Emotional Stability: Y, Agreeableness: Z, Conscientiousness: W, Openness: V.\"\n",
    "    )\n",
    "    inputs = tokenizer_t5(input_text, return_tensors=\"pt\", truncation=True, max_length=512).to(device)\n",
    "    outputs = model_t5.generate(**inputs, max_length=50, num_return_sequences=1)\n",
    "    prediction = tokenizer_t5.decode(outputs[0], skip_special_tokens=True)\n",
    "    return prediction\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 4. Parse the predicted scores from the model's output.\n",
    "def parse_scores(prediction):\n",
    "    parts = prediction.split(',')\n",
    "    scores = {}\n",
    "    for part in parts:\n",
    "        if ':' in part:\n",
    "            key, val = part.split(':', 1)\n",
    "            try:\n",
    "                scores[key.strip()] = float(val.strip())\n",
    "            except:\n",
    "                scores[key.strip()] = None\n",
    "    return scores\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "28d5d73f-a81e-4859-9209-83f63153cbdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------\n",
    "# 5. Main function to generate final personality feedback.\n",
    "def final_personality_feedback(q1, q2, q3, q4, q5):\n",
    "    # Use the fine-tuned model to predict aggregated scores.\n",
    "    prediction = predict_scores_from_questionnaire(q1, q2, q3, q4, q5)\n",
    "    scores = parse_scores(prediction)\n",
    "    report = personality_report(scores)\n",
    "    # Format scores for display.\n",
    "    scores_str = \", \".join([f\"{trait}: {score}\" for trait, score in scores.items()])\n",
    "    final_output = (\n",
    "        f\"Aggregated Scores: {scores_str}\\n\\n\"\n",
    "        f\"Personality Report:\\n{report}\\n\\n\"\n",
    "        f\"(Raw Model Prediction: {prediction})\"\n",
    "    )\n",
    "    return final_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b52fd22a-e8cc-46e3-921f-a7ef3b64b917",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7867\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7867/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/nageshs/miniconda3/lib/python3.12/site-packages/gradio/queueing.py\", line 625, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/nageshs/miniconda3/lib/python3.12/site-packages/gradio/route_utils.py\", line 322, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/nageshs/miniconda3/lib/python3.12/site-packages/gradio/blocks.py\", line 2051, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/nageshs/miniconda3/lib/python3.12/site-packages/gradio/blocks.py\", line 1598, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/nageshs/miniconda3/lib/python3.12/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/nageshs/miniconda3/lib/python3.12/site-packages/anyio/_backends/_asyncio.py\", line 2461, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"/Users/nageshs/miniconda3/lib/python3.12/site-packages/anyio/_backends/_asyncio.py\", line 962, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/nageshs/miniconda3/lib/python3.12/site-packages/gradio/utils.py\", line 883, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/k0/h7lvf47d4z31ztdbyt2m05rr0000gn/T/ipykernel_11795/237736081.py\", line 7, in final_personality_feedback\n",
      "    report = personality_report(scores)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/k0/h7lvf47d4z31ztdbyt2m05rr0000gn/T/ipykernel_11795/1616725858.py\", line 19, in personality_report\n",
      "    if scores.get('Extraversion', 0) < 3:\n",
      "       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "TypeError: '<' not supported between instances of 'NoneType' and 'int'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --------------------------------------------------\n",
    "# 6. Build the Gradio Interface with free-form questionnaire inputs.\n",
    "iface = gr.Interface(\n",
    "    fn=final_personality_feedback,\n",
    "    inputs=[\n",
    "        gr.Textbox(lines=3, label=\"Question 1: Describe your social life (Extraversion)\"),\n",
    "        gr.Textbox(lines=3, label=\"Question 2: How do you manage stress and emotions (Emotional Stability)\"),\n",
    "        gr.Textbox(lines=3, label=\"Question 3: Describe your organizational habits (Conscientiousness)\"),\n",
    "        gr.Textbox(lines=3, label=\"Question 4: Tell us about your interactions with others (Agreeableness)\"),\n",
    "        gr.Textbox(lines=3, label=\"Question 5: How open are you to new experiences (Openness)\")\n",
    "    ],\n",
    "    outputs=\"text\",\n",
    "    title=\"Personality Feedback Generator\",\n",
    "    description=(\n",
    "        \"Answer the questions in your own words. Your responses will be processed by a fine-tuned model \"\n",
    "        \"to predict aggregated personality scores, which are then used to generate a detailed personality report.\"\n",
    "    )\n",
    ")\n",
    "\n",
    "iface.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf3555c-f4b3-4b7a-acb3-ffc5b41fd91b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
